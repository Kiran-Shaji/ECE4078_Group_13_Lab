# estimate the pose of target objects detected
import numpy as np
import json
import os
import ast
import cv2
from YOLO.detector import Detector


# list of target fruits and vegs types
# Make sure the names are the same as the ones used in your YOLO model
TARGET_TYPES = ['Orange', 'Lemon', 'Lime', 'Tomato', 'Capsicum', 'Potato', 'Pumpkin', 'Garlic']


def estimate_pose(camera_matrix, obj_info, robot_pose):
    """
    function:
        estimate the pose of a target based on size and location of its bounding box and the corresponding robot pose
    input:
        camera_matrix: list, the intrinsic matrix computed from camera calibration (read from 'param/intrinsic.txt')
            |f_x, s,   c_x|
            |0,   f_y, c_y|
            |0,   0,   1  |
            (f_x, f_y): focal length in pixels
            (c_x, c_y): optical centre in pixels
            s: skew coefficient (should be 0 for PenguinPi)
        obj_info: list, an individual bounding box in an image (generated by get_bounding_box, [label,[x,y,width,height]])
        robot_pose: list, pose of robot corresponding to the image (read from 'lab_output/images.txt', [x,y,theta])
    output:
        target_pose: dict, prediction of target pose
    """
    # read in camera matrix (from camera calibration results)
    focal_length = camera_matrix[0][0]

    # there are 8 possible types of fruits and vegs
    ######### Replace with your codes #########
    # TODO: measure actual sizes of targets [width, depth, height] and update the dictionary of true target dimensions
    target_dimensions_dict = {'Orange': [0.082,0.084,0.078], 'Lemon': [0.08,0.053,0.051], 
                              'Lime': [0.08,0.053,0.0535], 'Tomato': [0.074,0.074,0.073], 
                              'Capsicum': [0.09,0.08,0.1], 'Potato': [0.1,0.072,0.062], 
                              'Pumpkin': [0.085,0.085,0.079], 'Garlic': [0.068,0.062,0.075]}
#########

    # estimate target pose using bounding box and robot pose
    target_class = obj_info[0]     # get predicted target label of the box
    target_box = obj_info[1]       # get bounding box measures: [x,y,width,height]
    true_height = target_dimensions_dict[target_class][2]   # look up true height of by class label

    # compute pose of the target based on bounding box info, true object height, and robot's pose
    pixel_height = target_box[3]
    pixel_center = target_box[0]
    distance = true_height/pixel_height * focal_length  # estimated distance between the robot and the centre of the image plane based on height
    # training image size 320x240p
    image_width = 320 # change this if your training image is in a different size (check details of pred_0.png taken by your robot)
    x_shift = image_width/2 - pixel_center              # x distance between bounding box centre and centreline in camera view
    theta = np.arctan(x_shift/focal_length)     # angle of object relative to the robot
    ang = theta + robot_pose[2]     # angle of object in the world frame
    
   # relative object location
    distance_obj = distance/np.cos(theta) # relative distance between robot and object
    x_relative = distance_obj * np.cos(theta) # relative x pose
    y_relative = distance_obj * np.sin(theta) # relative y pose
    relative_pose = {'x': x_relative, 'y': y_relative}
    #print(f'relative_pose: {relative_pose}')

    # location of object in the world frame using rotation matrix
    delta_x_world = x_relative * np.cos(robot_pose[2]) - y_relative * np.sin(robot_pose[2])
    delta_y_world = x_relative * np.sin(robot_pose[2]) + y_relative * np.cos(robot_pose[2])
    # add robot pose with delta target pose
    target_pose = {'y': (robot_pose[1]+delta_y_world)[0],
                   'x': (robot_pose[0]+delta_x_world)[0]}
    #print(f'delta_x_world: {delta_x_world}, delta_y_world: {delta_y_world}')
    #print(f'target_pose: {target_pose}')

    return target_pose


def merge_estimations(target_pose_dict):
    """
    function:
        merge estimations of the same target
    input:
        target_pose_dict: dict, generated by estimate_pose
    output:
        target_est: dict, target pose estimations after merging
    """
    target_est = {}

    ######### Replace with your codes #########
    # TODO: replace it with a solution to merge the multiple occurrences of the same class type (e.g., by a distance threshold)
    distance_threshold = 0.1
    
    target_est = {}

    for key, pose in target_pose_dict.items():
        # Check if there is already a similar target in target_est, written by GEN AI CHAT GPT-3.5
        merged = False
        for existing_key, existing_pose in target_est.items():
            # Calculate the distance between the new and existing poses
            distance = np.sqrt((pose['x'] - existing_pose['x'])**2 + (pose['y'] - existing_pose['y'])**2)

            if distance < distance_threshold:
                # Merge the poses by averaging their positions
                merged_pose = {
                    'x': (pose['x'] + existing_pose['x']) / 2,
                    'y': (pose['y'] + existing_pose['y']) / 2
                }
                target_est[existing_key] = merged_pose
                merged = True
                break

        if not merged:
            # If no similar target was found, add it as a new target
            target_est[key] = pose

    # target_est = target_pose_dict
    #########

    return target_est

def parse_slam_map(fname: str) -> dict:
    with open(fname, 'r') as fd:
        usr_dict = json.load(fd)
    aruco_dict = {}
    for (i, tag) in enumerate(usr_dict['taglist']):
        aruco_dict[tag] = np.reshape([usr_dict['map'][0][i], usr_dict['map'][1][i]], (2, 1))
    
    return aruco_dict


def parse_object_map(fname):
    with open(fname, 'r') as fd:
        usr_dict = json.load(fd)
    target_dict = {}

    for key in usr_dict:
        object_type = key.split('_')[0]
        if object_type not in target_dict:
            target_dict[object_type] = np.array([[usr_dict[key]['x'], usr_dict[key]['y']]])
        else:
            target_dict[object_type] = np.append(target_dict[object_type], [[usr_dict[key]['x'], usr_dict[key]['y']]], axis=0)

    #print(f'Estimated target poses: {target_dict}')
                    
    return target_dict
# main loop
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))     # get current script directory (TargetPoseEst.py)

    # read in camera matrix
    fileK = f'{script_dir}/calibration/param/intrinsic.txt'
    camera_matrix = np.loadtxt(fileK, delimiter=',')

    # init YOLO model
    model_path = f'{script_dir}/YOLO/model/yolov8_model.pt'
    yolo = Detector(model_path)

    # create a dictionary of all the saved images with their corresponding robot pose
    image_poses = {}
    with open(f'{script_dir}/lab_output/images - Copy.txt') as fp:
        for line in fp.readlines():
            pose_dict = ast.literal_eval(line)
            image_poses[pose_dict['imgfname']] = pose_dict['pose']

    # estimate pose of targets in each image
    target_pose_dict = {}
    detected_type_list = []
    for image_path in image_poses.keys():
        input_image = cv2.imread(image_path)
        bounding_boxes, bbox_img = yolo.detect_single_image(input_image)
        # cv2.imshow('bbox', bbox_img)
        # cv2.waitKey(0)
        robot_pose = image_poses[image_path]

        for detection in bounding_boxes:
            # count the occurrence of each target type
            occurrence = detected_type_list.count(detection[0])
            target_pose_dict[f'{detection[0].lower()}_{occurrence}'] = estimate_pose(camera_matrix, detection, robot_pose)

            detected_type_list.append(detection[0])

    # merge the estimations of the targets so that there are at most 3 estimations of each target type
    target_est = {}
    target_est = merge_estimations(target_pose_dict)
    formatted_aruco = {}
    # change each string to lowercase in the target_est dictionary
    # target_est = {fruits.lower(): targets for fruits, targets in target_est.items()}
    #print(target_est)
    # save target pose estimations
    with open(f'{script_dir}/lab_output/targets.txt', 'w') as fo:
        json.dump(target_est, fo, indent=4)


    aruco_est = parse_slam_map('lab_output/slam - Copy.txt')
    counter = 0
    for key, value in aruco_est.items():
        if key < 11:
            formatted_aruco[f"aruco{key}_0"] =  {"x": float(value[0]) , "y": float(value[1]) } 
        
    

    object_est = parse_object_map('lab_output/targets.txt')
    formatted_fruit = {}
    for key, values in object_est.items():
        for idx, value in enumerate(values):
            formatted_fruit[f"{key}_{idx}"] = {"x": float(value[0]), "y": float(value[1])}

    

    result = {**formatted_aruco, **formatted_fruit}
    #convert result to string
    

    with open("M5_Map_Output.txt", "w") as file:
        file.write(json.dumps(result))
    

    print('Estimations saved!')

    source_directory = 'lab_output'
    destination_directory = os.path.join(source_directory, 'Final_maps_113')

    files_to_copy = [('slam.txt', 'slam_run1_113.txt'), ('targets.txt', 'targets_run1_113.txt')]

    # Copy the files to the destination directory
    for source_file, new_name in files_to_copy:
        source_path = os.path.join(source_directory, source_file)
        destination_path = os.path.join(destination_directory, new_name)
        
        with open(source_path, 'r') as source_file:
            with open(destination_path, 'w') as dest_file:
                dest_file.write(source_file.read())
        
        

    print("saved in folder")
